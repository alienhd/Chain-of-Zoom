# Chain-of-Zoom: Extreme Super-Resolution via Scale Autoregression and Preference Alignment

This repository is the official implementation of [Chain-of-Zoom: Extreme Super-Resolution via Scale Autoregression and Preference Alignment](https://arxiv.org/abs/2505.18600), led by

[Bryan Sangwoo Kim](https://scholar.google.com/citations?user=ndWU-84AAAAJ&hl=en), [Jeongsol Kim](https://jeongsol.dev/), [Jong Chul Ye](https://bispl.weebly.com/professor.html)

![main figure](assets/teaser.jpg)

[![Project Website](https://img.shields.io/badge/Project-Website-blue)](https://bryanswkim.github.io/chain-of-zoom/)
[![arXiv](https://img.shields.io/badge/arXiv-2505.18600-b31b1b.svg)](https://arxiv.org/abs/2505.18600)

---
## üî• Summary

Modern single-image super-resolution (SISR) models deliver photo-realistic results at the scale factors on which they are trained, but show notable drawbacks:

1. **Blur and artifacts** when pushed to magnify beyond its training regime
2. **High computational costs and inefficiency** of retraining models when we want to magnify further

This brings us to the fundamental question: \
_How can we effectively utilize super-resolution models to explore much higher resolutions than they were originally trained for?_

We address this via **Chain-of-Zoom** üîé, a model-agnostic framework that factorizes SISR into an autoregressive chain of intermediate scale-states with multi-scale-aware prompts.
CoZ repeatedly re-uses a backbone SR model, decomposing the conditional probability into tractable sub-problems to achieve extreme resolutions without additional training.
Because visual cues diminish at high magnifications, we augment each zoom step with multi-scale-aware text prompts generated by a prompt extractor VLM.
This prompt extractor can be fine-tuned through GRPO with a critic VLM to further align text guidance towards human preference.

## üóì Ô∏èNews
- [May 2025] Code and paper are uploaded.

## üõ†Ô∏è Setup
First, create your environment. We recommend using the following commands. 

```
git clone https://github.com/bryanswkim/Chain-of-Zoom.git
cd Chain-of-Zoom

conda create -n coz python=3.10
conda activate coz
pip install -r requirements.txt
```

## ‚è≥ Models

|Models|Checkpoints|
|:---------|:--------|
|Stable Diffusion v3|[Hugging Face](https://huggingface.co/stabilityai/stable-diffusion-3-medium)
|Qwen2.5-VL-3B-Instruct|[Hugging Face](https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct)
|RAM|[Hugging Face](https://huggingface.co/spaces/xinyu1205/recognize-anything/blob/main/ram_swin_large_14m.pth)

## üåÑ Example
You can quickly check the results of using **CoZ** with the following example:
```
python inference_coz.py \
  -i samples \
  -o inference_results/coz_vlmprompt \
  --rec_type recursive_multiscale \
  --prompt_type vlm \
  --lora_path ckpt/SR_LoRA/model_20001.pkl \
  --vae_path ckpt/SR_VAE/vae_encoder_20001.pt \
  --pretrained_model_name_or_path 'stabilityai/stable-diffusion-3-medium-diffusers' \
  --ram_ft_path ckpt/DAPE/DAPE.pth \
  --ram_path ckpt/RAM/ram_swin_large_14m.pth \
```
Which will give a result like below:

![main figure](assets/example_result.png)

## üî¨ Efficient Memory
Using ```--efficient_memory``` allows CoZ to run on a single GPU with 24GB VRAM, but highly increases inference time due to offloading. \
We recommend using two GPUs.

## Model Quantization (Experimental)

This project includes an experimental feature for quantizing model components (specifically text encoders) to ONNX format using static quantization with `optimum[onnxruntime-gpu]`. This can potentially reduce memory usage and improve inference speed, especially on CPU or compatible GPU hardware, but may also impact model accuracy.

**Current Status:**
*   Quantization is implemented for `text_enc_1`, `text_enc_2`, and `text_enc_3`.
*   Quantization for `transformer`, `vae_encoder`, and `vae_decoder` is **not yet implemented**.
*   The process relies on Hugging Face's `optimum` library and ONNX Runtime.

**How to Quantize Models:**

1.  **Ensure Dependencies:** Make sure you have the necessary packages installed, including `optimum` and `onnxruntime-gpu` (or `onnxruntime` if you only have CPU). These should be covered if you installed via `requirements.txt` after the quantization updates.
    ```bash
    pip install optimum[onnxruntime-gpu]
    ```

2.  **Run the Quantization Script:**
    Use the `quantize_model.py` script to perform quantization. You need to specify the model component you wish to quantize.
    ```bash
    python quantize_model.py --model_component text_enc_1 --pretrained_model_name_or_path stabilityai/stable-diffusion-3-medium-diffusers --output_dir ckpt/quantized
    ```
    *   `--model_component`: Choose from `text_enc_1`, `text_enc_2`, `text_enc_3`.
    *   `--pretrained_model_name_or_path`: Specify the base model to load components from (e.g., `stabilityai/stable-diffusion-3-medium-diffusers`).
    *   `--output_dir`: Directory where the quantized ONNX models will be saved (e.g., `ckpt/quantized`). The script will create subdirectories for each component (e.g., `ckpt/quantized/text_enc_1/text_enc_1_quantized`).
    *   `--num_calibration_samples`: Number of sample prompts to use for calibration (default is 10). More samples might improve quantization quality but will take longer.
    *   `--device_for_quant_script`: Device to run the quantization script on (default is `cpu`). ONNX export and quantization often work best from CPU.

    Repeat the command for each text encoder you wish to quantize:
    ```bash
    python quantize_model.py --model_component text_enc_2 ...
    python quantize_model.py --model_component text_enc_3 ...
    ```

**How to Use Quantized Models During Inference:**

1.  **Quantize First:** Follow the steps above to generate the quantized ONNX model files.
2.  **Use the `--quantize` Flag:** When running `inference_coz.py`, add the `--quantize` flag.
    ```bash
    python inference_coz.py \
      -i samples \
      -o inference_results/coz_quantized_vlmprompt \
      --rec_type recursive_multiscale \
      --prompt_type vlm \
      --lora_path ckpt/SR_LoRA/model_20001.pkl \
      --vae_path ckpt/SR_VAE/vae_encoder_20001.pt \
      --pretrained_model_name_or_path 'stabilityai/stable-diffusion-3-medium-diffusers' \
      --ram_ft_path ckpt/DAPE/DAPE.pth \
      --ram_path ckpt/RAM/ram_swin_large_14m.pth \
      --quantize
    ```
    The script will attempt to load the quantized versions of `text_enc_1`, `text_enc_2`, and `text_enc_3` from the `ckpt/quantized/<component_name>/<component_name_quantized>` directory. If a quantized model is not found or fails to load, it will fall back to using the original PyTorch version for that component.

**Notes and Limitations:**
*   **Experimental:** This feature is experimental. Quantization might lead to changes in output quality or, in some cases, errors if the ONNX model is not perfectly compatible.
*   **Calibration Data:** The current calibration process for text encoders uses a small, fixed set of generic prompts. For optimal performance, this calibration data should ideally be more diverse and representative of the actual prompts you intend to use.
*   **T5 Encoder (`text_enc_3`):** Quantizing T5-based models can be complex. The script attempts to use the Hugging Face name (`google/t5-v1_1-xl`) for `text_enc_3`. If issues arise, manual ONNX export and specific quantization strategies for T5 might be needed.
*   **Performance:** The actual performance benefits (speed, memory) will vary depending on your hardware (CPU, GPU type) and the specifics of the ONNX Runtime execution providers.
*   **Error Handling:** If a quantized component fails to load or run, the inference script is designed to fall back to the original PyTorch component, but this fallback might not cover all error scenarios.

## Running Inference Scripts on Windows

When running the inference scripts (`inference_coz.py`) on Windows, you might encounter issues related to file paths and module resolution. Here are some common points to check:

1.  **File Paths:**
    *   The scripts have been updated to use `os.path.join` for constructing most file and directory paths, which should improve compatibility with Windows.
    *   Ensure that paths provided in command-line arguments (e.g., for `--input_image`, `--output_dir`, model paths) use backslashes (`\`) as separators or forward slashes (`/`) if your shell environment (like Git Bash) handles the conversion.
    *   If using absolute paths, make sure they are correct for your Windows environment (e.g., `C:\Users\YourUser\Project\data`).

2.  **Module Resolution (PYTHONPATH):**
    *   If you encounter `ModuleNotFoundError` for project-local modules (like `osediff_sd3`, `ram`, etc.), it might be due to Python not finding them.
    *   You can set the `PYTHONPATH` environment variable to include the project's root directory. For example, if your project is in `C:\Projects\VisionCoz`, you can set `PYTHONPATH` to this directory.
        *   In Command Prompt: `set PYTHONPATH=C:\Projects\VisionCoz` (for the current session)
        *   In PowerShell: `$env:PYTHONPATH="C:\Projects\VisionCoz"` (for the current session)
        *   To set it permanently, search for "environment variables" in Windows settings.
    *   Alternatively, ensure you are running the scripts from the project's root directory. The scripts include `sys.path.append(os.getcwd())` which should help if the current working directory is the project root.

3.  **xformers on Windows:**
    *   The `xformers` library, which provides memory-efficient attention, can sometimes be challenging to install or run on Windows.
    *   The inference script `osediff_sd3.py` now includes a try-except block when attempting to `pipe.enable_xformers_memory_efficient_attention()`. If it fails, it will print a message and proceed without it. Performance might be slightly impacted, or memory usage might be higher.
    *   If you wish to use `xformers`, refer to the official xformers documentation or community guides for installation on Windows. Pre-built wheels might be available for your Python and CUDA versions.

4.  **Dependencies:**
    *   Ensure all dependencies from `requirements.txt` are installed correctly in your Python environment. It's recommended to use a virtual environment.
        ```bash
        python -m venv venv
        venv\Scripts\activate
        pip install -r requirements.txt
        ```

5.  **Command Syntax:**
    *   When running Python scripts, ensure you use the `python` command:
        ```bash
        python inference_coz.py --input_image "path\to\your\image_or_folder" --output_dir "path\to\your\output" --prompt "your prompt" ... [other arguments]
        ```

By checking these points, you should be able to run the inference scripts more smoothly on a Windows system.

## üìù Citation
If you find our method useful, please cite as below or leave a star to this repository.

```
@article{kim2025chain,
  title={Chain-of-Zoom: Extreme Super-Resolution via Scale Autoregression and Preference Alignment},
  author={Kim, Bryan Sangwoo and Kim, Jeongsol and Ye, Jong Chul},
  journal={arXiv preprint arXiv:2505.18600},
  year={2025}
}
```

## ü§ó Acknowledgements
We thank the authors of [OSEDiff](https://github.com/cswry/OSEDiff) for sharing their awesome work!

> [!note]
> This work is currently in the preprint stage, and there may be some changes to the code.
