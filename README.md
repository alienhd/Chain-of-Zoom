# Chain-of-Zoom: Extreme Super-Resolution via Scale Autoregression and Preference Alignment

This repository is the official implementation of [Chain-of-Zoom: Extreme Super-Resolution via Scale Autoregression and Preference Alignment](https://arxiv.org/abs/2505.18600), led by

[Bryan Sangwoo Kim](https://scholar.google.com/citations?user=ndWU-84AAAAJ&hl=en), [Jeongsol Kim](https://jeongsol.dev/), [Jong Chul Ye](https://bispl.weebly.com/professor.html)

![main figure](assets/teaser.jpg)

[![Project Website](https://img.shields.io/badge/Project-Website-blue)](https://bryanswkim.github.io/chain-of-zoom/)
[![arXiv](https://img.shields.io/badge/arXiv-2505.18600-b31b1b.svg)](https://arxiv.org/abs/2505.18600)

---
## ðŸ”¥ Summary

Modern single-image super-resolution (SISR) models deliver photo-realistic results at the scale factors on which they are trained, but show notable drawbacks:

1. **Blur and artifacts** when pushed to magnify beyond its training regime
2. **High computational costs and inefficiency** of retraining models when we want to magnify further

This brings us to the fundamental question: \
_How can we effectively utilize super-resolution models to explore much higher resolutions than they were originally trained for?_

We address this via **Chain-of-Zoom** ðŸ”Ž, a model-agnostic framework that factorizes SISR into an autoregressive chain of intermediate scale-states with multi-scale-aware prompts.
CoZ repeatedly re-uses a backbone SR model, decomposing the conditional probability into tractable sub-problems to achieve extreme resolutions without additional training.
Because visual cues diminish at high magnifications, we augment each zoom step with multi-scale-aware text prompts generated by a prompt extractor VLM.
This prompt extractor can be fine-tuned through GRPO with a critic VLM to further align text guidance towards human preference.

## ðŸ—“ ï¸News
- [May 2025] Code and paper are uploaded.

## ðŸ› ï¸ Setup
First, create your environment. We recommend using the following commands. 

```
git clone https://github.com/bryanswkim/Chain-of-Zoom.git
cd Chain-of-Zoom

conda create -n coz python=3.10
conda activate coz
pip install -r requirements.txt
```

## â³ Models

|Models|Checkpoints|
|:---------|:--------|
|Stable Diffusion v3|[Hugging Face](https://huggingface.co/stabilityai/stable-diffusion-3-medium)
|Qwen2.5-VL-3B-Instruct|[Hugging Face](https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct)
|RAM|[Hugging Face](https://huggingface.co/spaces/xinyu1205/recognize-anything/blob/main/ram_swin_large_14m.pth)

## ðŸŒ„ Example
You can quickly check the results of using **CoZ** with the following example:
```
python inference_coz.py \
  -i samples \
  -o inference_results/coz_vlmprompt \
  --rec_type recursive_multiscale \
  --prompt_type vlm \
  --lora_path ckpt/SR_LoRA/model_20001.pkl \
  --vae_path ckpt/SR_VAE/vae_encoder_20001.pt \
  --pretrained_model_name_or_path 'stabilityai/stable-diffusion-3-medium-diffusers' \
  --ram_ft_path ckpt/DAPE/DAPE.pth \
  --ram_path ckpt/RAM/ram_swin_large_14m.pth \
```
Which will give a result like below:

![main figure](assets/example_result.png)

## ðŸ”¬ Efficient Memory
Using ```--efficient_memory``` allows CoZ to run on a single GPU with 24GB VRAM, but highly increases inference time due to offloading. \
We recommend using two GPUs.

## Running Inference Scripts on Windows

When running the inference scripts (`inference_coz.py`) on Windows, you might encounter issues related to file paths and module resolution. Here are some common points to check:

1.  **File Paths:**
    *   The scripts have been updated to use `os.path.join` for constructing most file and directory paths, which should improve compatibility with Windows.
    *   Ensure that paths provided in command-line arguments (e.g., for `--input_image`, `--output_dir`, model paths) use backslashes (`\`) as separators or forward slashes (`/`) if your shell environment (like Git Bash) handles the conversion.
    *   If using absolute paths, make sure they are correct for your Windows environment (e.g., `C:\Users\YourUser\Project\data`).

2.  **Module Resolution (PYTHONPATH):**
    *   If you encounter `ModuleNotFoundError` for project-local modules (like `osediff_sd3`, `ram`, etc.), it might be due to Python not finding them.
    *   You can set the `PYTHONPATH` environment variable to include the project's root directory. For example, if your project is in `C:\Projects\VisionCoz`, you can set `PYTHONPATH` to this directory.
        *   In Command Prompt: `set PYTHONPATH=C:\Projects\VisionCoz` (for the current session)
        *   In PowerShell: `$env:PYTHONPATH="C:\Projects\VisionCoz"` (for the current session)
        *   To set it permanently, search for "environment variables" in Windows settings.
    *   Alternatively, ensure you are running the scripts from the project's root directory. The scripts include `sys.path.append(os.getcwd())` which should help if the current working directory is the project root.

3.  **xformers on Windows:**
    *   The `xformers` library, which provides memory-efficient attention, can sometimes be challenging to install or run on Windows.
    *   The inference script `osediff_sd3.py` now includes a try-except block when attempting to `pipe.enable_xformers_memory_efficient_attention()`. If it fails, it will print a message and proceed without it. Performance might be slightly impacted, or memory usage might be higher.
    *   If you wish to use `xformers`, refer to the official xformers documentation or community guides for installation on Windows. Pre-built wheels might be available for your Python and CUDA versions.

4.  **Dependencies:**
    *   Ensure all dependencies from `requirements.txt` are installed correctly in your Python environment. It's recommended to use a virtual environment.
        ```bash
        python -m venv venv
        venv\Scripts\activate
        pip install -r requirements.txt
        ```

5.  **Command Syntax:**
    *   When running Python scripts, ensure you use the `python` command:
        ```bash
        python inference_coz.py --input_image "path\to\your\image_or_folder" --output_dir "path\to\your\output" --prompt "your prompt" ... [other arguments]
        ```

By checking these points, you should be able to run the inference scripts more smoothly on a Windows system.

## ðŸ“ Citation
If you find our method useful, please cite as below or leave a star to this repository.

```
@article{kim2025chain,
  title={Chain-of-Zoom: Extreme Super-Resolution via Scale Autoregression and Preference Alignment},
  author={Kim, Bryan Sangwoo and Kim, Jeongsol and Ye, Jong Chul},
  journal={arXiv preprint arXiv:2505.18600},
  year={2025}
}
```

## ðŸ¤— Acknowledgements
We thank the authors of [OSEDiff](https://github.com/cswry/OSEDiff) for sharing their awesome work!

> [!note]
> This work is currently in the preprint stage, and there may be some changes to the code.
